{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HTTP_PROXY'] = \"http://127.0.0.1:20171\"\n",
    "os.environ['HTTPS_PROXY'] = \"http://127.0.0.1:20171\"\n",
    "os.environ['http_proxy'] = \"http://127.0.0.1:20171\"\n",
    "os.environ['https_proxy'] = \"http://127.0.0.1:20171\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv('GC_QUOTE_API_CREDENTIALS')\n",
    "os.getenv('HTTP_PROXY')\n",
    "os.getenv('HTTPS_PROXY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\"../.env\")\n",
    "key = config.get('GC_QUOTE_API_CREDENTIALS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas_gbq\n",
    "from google.oauth2 import service_account\n",
    "credentials = service_account.Credentials.from_service_account_file(\"../\" + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client(credentials=credentials)\n",
    "\n",
    "QUERY = \"\"\"\n",
    "\t\tselect *\n",
    "\t\tfrom `quote-api-365206.report.ride_dispatch_v` r\n",
    "\t\twhere dispatch_amount_net_usd is not null\n",
    "        limit 200\n",
    "\t\t\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_DISINCT = \"\"\"\n",
    "\t\tselect count (*)\n",
    "\t\tfrom `quote-api-365206.report.ride_dispatch_v` r\n",
    "\t\twhere trip_type is null\n",
    "        limit 200\n",
    "\t\t\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $HTTP_PROXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_job = client.query(QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pandas_gbq.read_gbq(QUERY,credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_trip_type = pandas_gbq.read_gbq(QUERY_DISINCT,credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_ROUTE = \"\"\"\n",
    "        SELECT r.*, p1.lat from_lat, p1.lng from_lng, p2.lat to_lat, p2.lng to_lng\n",
    "        FROM `report.ride_dispatch_v` r\n",
    "        left JOIN `report.dim_place` p1\n",
    "        ON r.pickup_place_id=p1.id\n",
    "        INNER JOIN `report.dim_place` p2\n",
    "        ON r.pickup_place_id=p2.id\n",
    "        ORDER BY ride_id DESC\n",
    "        limit 2;\n",
    "\t\t\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_route = pandas_gbq.read_gbq(QUERY_ROUTE,credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_route.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_route.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandera as pa\n",
    "\n",
    "# data to validate\n",
    "df = pd.DataFrame({\n",
    "    \"column1\": [1, 4, 0, 10, 9, 11],\n",
    "    \"column2\": [-1.3, -1.4, -2.9, -10.1, -20.4, -2.0],\n",
    "    \"column3\": [\"value_1\", \"value_2\", \"value_3\", \"value_2\", \"value_1\", \"value_2\"],\n",
    "})\n",
    "\n",
    "# define schema\n",
    "schema = pa.DataFrameSchema({\n",
    "    \"column1\": pa.Column(int, checks=pa.Check.le(10)),\n",
    "    \"column2\": pa.Column(float, checks=pa.Check.lt(-1.2)),\n",
    "    \"column3\": pa.Column(str, checks=[\n",
    "        pa.Check.str_startswith(\"value_\"),\n",
    "        # define custom checks as functions that take a series as input and\n",
    "        # outputs a boolean or boolean Series\n",
    "        pa.Check(lambda s: s.str.split(\"_\", expand=True).shape[1] == 2)\n",
    "    ]),\n",
    "})\n",
    "\n",
    "validated_df = schema(df)\n",
    "print(validated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandera.typing import Series\n",
    "\n",
    "class Schema(pa.DataFrameModel):\n",
    "\n",
    "    column1: int = pa.Field(le=10)\n",
    "    column2: float = pa.Field(lt=-1.2)\n",
    "    column3: str = pa.Field(str_startswith=\"value_\")\n",
    "\n",
    "    @pa.check(\"column3\")\n",
    "    def column_3_check(cls, series: Series[str]) -> Series[bool]:\n",
    "        \"\"\"Check that column3 values have two elements after being split with '_'\"\"\"\n",
    "        return series.str.split(\"_\", expand=True).shape[1] == 2\n",
    "\n",
    "Schema.validate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def validate_datetime(date_string, format_string):\n",
    "    \"\"\"\n",
    "    Validates if a string is a valid datetime according to the given format.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        datetime.strptime(date_string, format_string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def validate_datetime_in_iso_format(date_text):\n",
    "        try:\n",
    "            datetime.fromisoformat(date_text)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            # raise ValueError(\"Incorrect data format, should be YYYY-MM-DD\")\n",
    "            return False\n",
    "# Example usage\n",
    "date_strings = [\n",
    "    \"2023-12-25 24:00:00\",\n",
    "    \"2023-12-25 12:10:00\",\n",
    "    \"2023-12-25 23:00:00\",\n",
    "    \"2011-11-04\",\n",
    "    \"20111104\",\n",
    "    \"2011-11-04T00:05:23\",\n",
    "    \"2011-11-04T00:05:23Z\",\n",
    "    \"2011-11-04T00:05:23.283185\",\n",
    "    \"2011-11-04T00:05:23.283185+08:00\",\n",
    "    \"20111104T000523\",\n",
    "    \"20111104T000523.283185\",\n",
    "    \"2011-11-04T00:05:23+08:00\",\n",
    "    ]\n",
    "format_string = \"%Y-%m-%d\"\n",
    "\n",
    "for s in date_strings:\n",
    "    if validate_datetime_in_iso_format(s):\n",
    "        print(f\"{s} is Valid datetime string\")\n",
    "    else:\n",
    "        print(f\"{s} is Invalid datetime string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datetime_str = pd.DataFrame(date_strings)\n",
    "df_datetime_str.columns = ['datetime']\n",
    "# df_datetime_str\n",
    "df_datetime_str['validity'] = df_datetime_str.apply(lambda x: validate_datetime_in_iso_format(x.iloc[0]), axis=1)\n",
    "df_datetime_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datetime = df_datetime_str[df_datetime_str.apply(lambda x: validate_datetime_in_iso_format(x.iloc[0]), axis=1)]\n",
    "df_datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datetime['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_datetime_str.apply(lambda x: validate_datetime_in_iso_format(x.iloc[0]), axis=1)\n",
    "df_datetime1 = df_datetime.apply(lambda x: datetime.fromisoformat(x['datetime']), axis=1)\n",
    "df_datetime1.name = 'datetime'\n",
    "df_datetime1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(df_datetime1[7:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "def get_timezone_abbreviation(timezone_name):\n",
    "    timezone = pytz.timezone(timezone_name)\n",
    "    now = datetime.now(timezone)\n",
    "    return now.strftime(\"%Z\")\n",
    "\n",
    "def validate_timezone_in_iana(timezone_name):\n",
    "     return timezone_name in pytz.all_timezones\n",
    "\n",
    "time_zone_strings = [\n",
    "    'Asia/Shanghai',\n",
    "    'Asia/Mumbai',\n",
    "    'America/New_York',\n",
    "    'Europe/London', \n",
    "    'Eastern Standard Time', \n",
    "    'US/Eastern', \n",
    "    'America/Los_Angeles', \n",
    "    'Asia/Kolkata', \n",
    "    'Europe/London'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timezone_str = pd.DataFrame(time_zone_strings)\n",
    "df_timezone_str.columns = ['timezone']\n",
    "# df_datetime_str\n",
    "df_timezone_str['validity'] = df_timezone_str.apply(lambda x: validate_timezone_in_iana(x.iloc[0]), axis=1)\n",
    "df_timezone_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_timezone = df_timezone_str[df_timezone_str.apply(lambda x: validate_timezone_in_iana(x.iloc[0]), axis=1)]\n",
    "df_timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abbr = df_timezone.apply(lambda x: get_timezone_abbreviation(x.loc['timezone']), axis=1)\n",
    "df_abbr.name = 'abbr'\n",
    "df_abbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_timezone_str['validity'] = df_timezone_str.apply(lambda x: validate_timezone_in_iana(x.iloc[0]), axis=1)\n",
    "# df_timezone['abbr'] = df_timezone.apply(lambda x: get_timezone_abbreviation(x.iloc[0]), axis=1)\n",
    "# df_timezone.apply(lambda x: get_timezone_abbreviation(x.iloc[0]), axis=1)\n",
    "df_timezone.loc[:,'abbr'] = df_abbr\n",
    "df_timezone\n",
    "# df_timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_utc_offset = (df_timezone.apply(lambda x: datetime.now(pytz.timezone(x.loc['timezone'])), axis=1)\n",
    "                            .apply(lambda x: x.strftime('%z')))\n",
    "df_utc_offset.name = 'utc_offset'\n",
    "df_utc_offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_timezone.loc[:,'utc_offset'] = df_utc_offset\n",
    "df_timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tz in df_timezone.loc[:,'timezone']:\n",
    "    print(f\"{tz}: {datetime.now(pytz.timezone(tz)).strftime('%Z')}, {datetime.now(pytz.timezone(tz)).strftime('%z')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
