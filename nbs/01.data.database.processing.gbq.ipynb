{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.database.processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from pyasn1_modules.rfc3279 import id_fieldType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sqlalchemy import MetaData, create_engine, asc, desc, and_, or_, not_, case, extract, cast, Numeric, text, distinct, Column, update, bindparam, Engine\n",
    "from sqlalchemy.types import DateTime, Date, Time, String\n",
    "from sqlalchemy.schema import *\n",
    "from sqlalchemy.sql import func as F, Selectable, select, union, insert\n",
    "from sqlalchemy.dialects import registry\n",
    "from sqlalchemy.engine.row import Row\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy.orm import sessionmaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import requests\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# local table from local sqlite database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sql Lookup by join a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_eng = create_engine('sqlite:///../data/price_training_raw.db', echo=False)\n",
    "connection = sql_eng.connect()\n",
    "metadata = MetaData()\n",
    "Session = sessionmaker(bind=sql_eng)\n",
    "session = Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes_dispatches = pd.read_csv('../data/route_dispatches.csv',index_col=['route_start', 'route_end'])\n",
    "# routes_dispatches.sort_values(by=['count'], ascending=False, inplace=True)\n",
    "routes_dispatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# routes_dispatches.to_sql('route_dispatches', sql_eng, if_exists='replace')\n",
    "routes_dispatches.iloc[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Google BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from google.oauth2 import service_account\n",
    "import os\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "#| hide\n",
    "from google.cloud import bigquery\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv(\"../.env\")\n",
    "os.environ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_ods = os.getenv('GC_QUOTE_API_CREDENTIALS')\n",
    "credential_ods = service_account.Credentials.from_service_account_file(\"../\" + key_ods)\n",
    "key_test = os.getenv('GC_TEST_API_CREDENTIALS')\n",
    "credential_test = service_account.Credentials.from_service_account_file(\"../\" + key_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry.register('bigquery', 'sqlalchemy_bigquery', 'BigQueryDialect')\n",
    "engine_ods = create_engine('bigquery://quote-api-365206',\n",
    "                       credentials_path='../' + key_ods )\n",
    "\n",
    "metadata_ods = MetaData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine_test = create_engine('bigquery://elife-data-warehouse-test',\n",
    "engine_test = create_engine('bigquery://quote-api-365206',\n",
    "                       credentials_path='../' + key_test )\n",
    "\n",
    "metadata_test = MetaData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(engine_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_table(project_name: str, dataset_name: str, table_name: str, engine: Engine) -> Table:\n",
    "    table = Table(f'{project_name}.{dataset_name}.{table_name}', metadata, autoload_with=engine)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpz_lut_t = get_table('elife-data-warehouse-test', 'price', 'fixed_zone_routes ', engine_test).alias()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_dispatch_t = get_table('elife-data-warehouse-prod', 'ods', 'ride_dispatch', engine_ods).alias()\n",
    "ride_dispatch_t = ride_dispatch_t.alias()\n",
    "ride_ride_t = get_table('elife-data-warehouse-prod','ods', 'ride_ride', engine_ods)\n",
    "ride_ride_t = ride_ride_t.alias()\n",
    "ride_partner_tran_t = get_table('elife-data-warehouse-prod','ods', 'ride_partner_tran', engine_ods).alias()\n",
    "ride_partner_t = get_table('elife-data-warehouse-prod','ods', 'ride_partner', engine_ods).alias()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partner_id_t = select(ride_ride_t.c.id.label('ride_id'), ride_partner_tran_t.c.partner_id.label('partner_id'))\n",
    "partner_id_t = partner_id_t.select_from(ride_ride_t\n",
    "                                        .join(ride_partner_tran_t,\n",
    "                                              ride_ride_t.c.partner_tran_id == ride_partner_tran_t.c.id, isouter=True))\n",
    "partner_id_t = partner_id_t.alias()\n",
    "partner_t = select(partner_id_t.c.ride_id, partner_id_t.c.partner_id, ride_partner_t.c.name.label('partner'))\n",
    "partner_t = partner_t.select_from(partner_id_t\n",
    "                                  .join(ride_partner_t, partner_id_t.c.partner_id == ride_partner_t.c.id, isouter=True))\n",
    "partner_t = partner_t.alias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_ride_t0 = select(ride_ride_t)\n",
    "ride_ride_t0 = ride_ride_t0.limit(3)\n",
    "df = pd.read_sql(ride_ride_t0, engine_ods)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_dispatch_t0 = select(ride_dispatch_t)\n",
    "ride_dispatch_t0 = ride_dispatch_t0.limit(3)\n",
    "df = pd.read_sql(ride_dispatch_t0, engine_ods)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "x = routes_dispatches.iloc[0,1]\n",
    "l = ast.literal_eval(routes_dispatches.iloc[0,1])\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_dispatches_partner_t = select(\n",
    "    ride_dispatch_t.c.id.label('dispatch_id'),\n",
    "    ride_dispatch_t.c.amount.label('dispatch_amount'),\n",
    "    ride_dispatch_t.c.currency.label('dispatch_currency'),\n",
    "    partner_t.c.partner_id,\n",
    "    partner_t.c.partner,\n",
    ").where(ride_dispatch_t.c.id.in_(ast.literal_eval(routes_dispatches.iloc[0,1])))\n",
    "\n",
    "route_dispatches_partner_t = route_dispatches_partner_t.select_from(ride_dispatch_t\n",
    "                                                                    .join(partner_t, ride_dispatch_t.c.ride_id == partner_t.c.ride_id, isouter=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(route_dispatches_partner_t.limit(1000), engine_ods)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes_count = routes_dispatches['count']\n",
    "routes_count = routes_count.apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes_count.plot(figsize=(20,6))\n",
    "# routes_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_count = routes_count.cumsum()\n",
    "cumulative_count /= cumulative_count.iloc[-1]\n",
    "cumulative_count = cumulative_count.apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cumulative_count.plot(figsize=(20,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_routes = df.dropna(subset=['route_start', 'route_end'],how='all')\n",
    "# len(df_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_routes.set_index(['dispatch_id', 'route_end'], inplace=True)\n",
    "# df_routes.iloc[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = df.columns\n",
    "# cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cols = [c.name for c in query.subquery().columns]\n",
    "# result = [dict(zip(cols, row)) for row in df.to_numpy()]\n",
    "# # result = [row for row in df.to_numpy()]\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from concurrent import futures\n",
    "# from enum import Enum\n",
    "# \n",
    "# chunk_size = 1000\n",
    "# non_fpz_count = 0\n",
    "# max_concurrent = 24\n",
    "# \n",
    "# connection2 = sql_eng.connect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query1000= query.limit(1000)\n",
    "# chunk_size=100\n",
    "# raw_total_rows = 1000\n",
    "# # df = pd.read_sql(query1000, sql_eng)\n",
    "# # df\n",
    "# for i,chunk in enumerate(tqdm(pd.read_sql(query1000,connection,index_col='dispatch_id', chunksize=chunk_size), total=raw_total_rows//chunk_size+1, desc='Overall Progress')):\n",
    "#     # for chunk in tqdm(pd.read_csv(csv_file_path, chunksize=chunk_size), total=total_lines//chunk_size +1):\n",
    "#     # chunk.to_sql(\"price_training_labeled_2024_usd\", sql_eng, if_exists='append', index=True)\n",
    "#     dict_to_insert = [dict(zip(cols,row)) for row in chunk.to_numpy()]\n",
    "#     insert_stmt = insert(label_archive).values(dict_to_insert)\n",
    "#     session.execute(insert_stmt)\n",
    "#     session.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPZQueryStatus = Enum('FPZQueryStatus', 'JsonError KeyError DictError NoFixedPrice Success')\n",
    "# fpz = {'route_start': None, 'route_end': None}\n",
    "# def get_one_fpz(start_ltt_lp, start_lng_lp, end_ltt_lp, end_lng_lp) -> FPZQueryStatus:\n",
    "#     global fpz, df_lut\n",
    "#     idx = (start_ltt_lp, start_lng_lp, end_ltt_lp, end_lng_lp)\n",
    "#     try:\n",
    "#         fpz = dict(df_lut.loc[idx,['route_start', 'route_end']])\n",
    "#     except KeyError as e:\n",
    "#         fpz = {'route_start': None, 'route_end': None}\n",
    "#         return FPZQueryStatus.KeyError\n",
    "#     except Exception as exc:\n",
    "#         raise exc\n",
    "# \n",
    "#     return FPZQueryStatus.Success\n",
    "\n",
    "# for i,chunk in enumerate(tqdm(pd.read_sql(query,connection,index_col='dispatch_id', chunksize=chunk_size), total=raw_total_rows//chunk_size+1, desc='Overall Progress')):\n",
    "#     try:\n",
    "#         # pd.DataFrame(data=route_list, columns=['dispatch_id', 'route_start', 'route_end']).to_csv(result_csv,     mode='a', header=False)\n",
    "#         # if i<500:\n",
    "#         #     file_path = \"../data/price_training_labeled_2024_usd1.csv\"       \n",
    "#         # else:\n",
    "#         #     file_path = \"../data/price_training_labeled_2024_usd2.csv\"\n",
    "#         file_path = \"../data/price_training_labeled_2024_usd.csv\"\n",
    "#         with open(file_path, 'a') as f:\n",
    "#             # chunk.to_csv(f, header=f.tell()==0,chunksize=chunk_size)\n",
    "#             chunk.to_sql(result_table[0], sql_eng, if_exists='append', index=True)\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\r {e}\")\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = df_lut.iloc[:10,:]\n",
    "# df1\n",
    "# len(df1)\n",
    "# idx = df1.index.drop_duplicates(keep='first')\n",
    "# df1[~df1.index.duplicated(keep='first')]\n",
    "# df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df1 = df_lut.iloc[:10,:]\n",
    "# # df1\n",
    "# # len(df1)\n",
    "# # idx = df1.index.drop_duplicates(keep='first')\n",
    "# # df1[~df1.index.duplicated(keep='first')]\n",
    "# # df1\n",
    "# route_t = Table('route_lp', metadata, autoload_with=sql_eng)\n",
    "# route_t = select(route_t)\n",
    "# # noinspection PyUnboundLocalVariable\n",
    "# connection = sql_eng.connect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = route_t.limit(10)\n",
    "# df = pd.read_sql(s,sql_eng)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# from concurrent import futures\n",
    "# from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_lp_route_result = '../data/lp_route_result.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# non_fpz_count = 0\n",
    "# max_concurrent = 24\n",
    "# FPZQueryStatus = Enum('FPZQueryStatus', 'JsonError KeyError DictError NoFixedPrice Success')\n",
    "# fpz = {'route_start': None, 'route_end': None}\n",
    "# def get_one_fpz(start_ltt_lp, start_lng_lp, end_ltt_lp, end_lng_lp) -> FPZQueryStatus:\n",
    "#     global fpz, df_lut\n",
    "#     idx = (start_ltt_lp, start_lng_lp, end_ltt_lp, end_lng_lp)\n",
    "#     try:\n",
    "#         fpz = dict(df_lut.loc[idx,['route_start', 'route_end']])\n",
    "#     except KeyError as e:\n",
    "#         fpz = {'route_start': None, 'route_end': None}\n",
    "#         return FPZQueryStatus.KeyError\n",
    "#     except Exception as exc:\n",
    "#         raise exc\n",
    "#     \n",
    "#     return FPZQueryStatus.Success\n",
    "# \n",
    "# for i,chunk in enumerate(tqdm(pd.read_sql(route_t,connection,index_col='dispatch_id', chunksize=chunk_size), total=total_rows//chunk_size+1, desc='Overall Progress')):\n",
    "#     chunk = chunk.astype({'start_ltt_lp': float, 'start_lng_lp': float, 'end_ltt_lp': float, 'end_lng_lp': float, 'route_start': str, 'route_end': str})\n",
    "#     non_fpz_count_chunk = 0\n",
    "#     # get_fixed_zone_chunk(chunk)\n",
    "#     # for ind,r in chunk.iterrows():\n",
    "#     #     result = get_one_fpz(r['start_ltt_lp'], r['start_lng_lp'],r['end_ltt_lp'],r['end_lng_lp'])\n",
    "#         \n",
    "#     \n",
    "#     with ThreadPoolExecutor(max_workers=max_concurrent) as executor:\n",
    "#         to_do_map = {} # list[futures.Future] = [] \n",
    "#         # for ind,r in tqdm(chunk.iterrows(),total=chunk_size,desc='chunk progress', leave=False):\n",
    "#         for ind,r in chunk.iterrows():\n",
    "#             future = executor.submit(get_one_fpz, r['start_ltt_lp'], r['start_lng_lp'],r['end_ltt_lp'],r['end_lng_lp'])\n",
    "#             to_do_map[future] = ind\n",
    "#         done_iter = as_completed(to_do_map)\n",
    "#         done_iter = tqdm(done_iter, total=len(to_do_map), desc='chunk progress', leave=False)\n",
    "# \n",
    "#         for future in done_iter:\n",
    "#             try:\n",
    "#                 status = future.result()\n",
    "#             except Exception as e:\n",
    "#                 # print(f\"Exception: {e}\")\n",
    "#                 continue\n",
    "# \n",
    "#             if status == FPZQueryStatus.Success:\n",
    "#                 ind = to_do_map[future]\n",
    "#                 try:\n",
    "#                     chunk.at[ind, 'route_start'] = fpz['route_start']\n",
    "#                     chunk.at[ind, 'route_end'] = fpz['route_end']\n",
    "#                 except KeyError as e:\n",
    "#                     non_fpz_count_chunk += 1\n",
    "#                     # print(f\"\\r {e}: {non_fzp_count_chunk}/{chunk_size}\")\n",
    "#                     continue\n",
    "#                 except Exception as e:\n",
    "#                     non_fpz_count_chunk += 1\n",
    "#                     # print(f\"\\r {e}: {non_fzp_count_chunk}/{chunk_size}\")\n",
    "#                     continue\n",
    "#             else:\n",
    "#                 non_fpz_count_chunk += 1\n",
    "#                 # print(f\"\\r non fzp chunk: {non_fzp_count_chunk}/{chunk_size}\")\n",
    "#                 continue\n",
    "#         \n",
    "#     non_fpz_count += non_fpz_count_chunk\n",
    "#     scanned_row_number = (i+1)*chunk_size\n",
    "#     try:\n",
    "#         # pd.DataFrame(data=route_list, columns=['dispatch_id', 'route_start', 'route_end']).to_csv(result_csv,     mode='a', header=False)\n",
    "#         with open(csv_lp_route_result, 'a') as f:\n",
    "#             chunk.to_csv(f, header=f.tell()==0,chunksize=chunk_size)\n",
    "#         # chunk.to_csv(path_or_buf=csv_result_file_list[0],mode='a',chunksize=chunk_size)\n",
    "#         # chunk.to_sql(result_table[0], sql_eng, if_exists='append', index=True)\n",
    "#         print(f\"\\r Non_FZP count: {non_fpz_count}/{scanned_row_number}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\r {e}, Non_FZP count: {non_fpz_count}/{scanned_row_number} \")\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inserted two new columns for fixed price zone start_zone and end_zone when creating sql table from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # engine.execute('ALTER TABLE price_training_raw ADD COLUMN start_zone TEXT')\n",
    "# # engine.execute('ALTER TABLE price_training_raw ADD COLUMN end_zone TEXT')\n",
    "# start_zone_column = Column('start_zone', String)\n",
    "# end_zone_column = Column('end_zone', String)\n",
    "# add_column_op = AddColumn(start_zone_column, raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://j1j495o5pk.execute-api.us-east-2.amazonaws.com/upncoming/ride-pricings'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Query the table in chunks\n",
    "# query = session.query(raw)\n",
    "# chunk_size =1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_stmt = (\n",
    "#     update(raw)  # 'raw' is your table object\n",
    "#     .where(raw.c.dispatch_id == bindparam('b_dispatch_id'))\n",
    "#     .values(\n",
    "#         route_start=bindparam('route_start'),\n",
    "#         route_end=bindparam('route_end')\n",
    "#     )\n",
    "# )\n",
    "# print(batch_stmt)\n",
    "# compiled = batch_stmt.compile()\n",
    "# print(compiled.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_csv = '../data/dispatch_fixed_zones_all.csv'\n",
    "# for chunk in tqdm(pd.read_sql(query.statement, conn, index_col='dispatch_id', chunksize=chunk_size), total=total_rows//chunk_size+1, desc='Overall Processing'):\n",
    "#     # [chunk[r] for r in chunk]\n",
    "#     # l = [r for r in chunk.iterrows()]\n",
    "#     # l\n",
    "#     # print(chunk.dtypes)\n",
    "#     route_list = []\n",
    "#     for i,r in tqdm(chunk.iterrows(),total=chunk_size, desc='Chunk Processing', leave=False):\n",
    "#         # l = [i, r['start_ltt'], r['start_lng'], r['end_ltt'], r['end_lng']]\n",
    "#         # print(l)\n",
    "#         params = {\n",
    "#             'from_lat': r['start_ltt'],\n",
    "#             'from_lng': r['start_lng'],\n",
    "#             'to_lat': r['end_ltt'],\n",
    "#             'to_lng': r['end_lng'],\n",
    "#         }\n",
    "#         try:\n",
    "#             response = requests.get(url=url, params=params)\n",
    "#         except requests.exceptions.Timeout:\n",
    "#             print('Timeout')\n",
    "#             continue\n",
    "#         except requests.exceptions.TooManyRedirects:\n",
    "#             print('TooManyRedirects')\n",
    "#             continue\n",
    "#             # Tell the user their URL was bad and try a different one\n",
    "#         except requests.exceptions.RequestException as e:\n",
    "#             print('RequestException, Catastrophic error!')\n",
    "#             continue\n",
    "#             # catastrophic error. bail.\n",
    "#             # raise SystemExit(e)\n",
    "# \n",
    "#         except Exception as e:\n",
    "#             print(f\"request: {e}\")\n",
    "#             continue\n",
    "#         # print('2')\n",
    "#         try:\n",
    "#             res = response.json()\n",
    "#         except Exception as e:\n",
    "#             print(f\"json: {e}\")\n",
    "#             continue\n",
    "#         # print('3')\n",
    "#         try:\n",
    "#             fix_price_zones = res['fleets'][0]['vehicle_classes'][0]['price_detail']['base_pricing']['fix_price_detail']\n",
    "#         except KeyError as e:\n",
    "#             print(f\"No Fixed Price!\")\n",
    "#             continue\n",
    "#         except IndexError as e:\n",
    "#             print(\"IndexError for fix_price_zones\")\n",
    "#             continue\n",
    "#         except Exception as e:\n",
    "#             print(f\"dict: {e}\")\n",
    "#             continue\n",
    "#         # print('4')\n",
    "#         if not isinstance(fix_price_zones,dict):\n",
    "#             print(f\"No fix price: {fix_price_zones}\")\n",
    "#         else:\n",
    "#             try:\n",
    "#                 route_list.append((i, fix_price_zones['from'], fix_price_zones['to']))\n",
    "#                 # fix_zone_routes_list.append(route)\n",
    "#                 # chunk.at[i, 'route_start'] = fix_price_zones['from']\n",
    "#                 # chunk.at[i, 'route_end'] = fix_price_zones['to']\n",
    "#                 # ins = insert(fixed_zone_routes).values(\n",
    "#                 #     start=fix_price_zones['from'], end=fix_price_zones['from'], dispatch_id= pt        ._mapping['dispatch_id'])\n",
    "#                 # stmt = (\n",
    "#                 #     update(raw)  # 'raw' is your table object\n",
    "#                 #     .where(raw.c.dispatch_id == int(i))\n",
    "#                 #     .values(\n",
    "#                 #         route_start = fix_price_zones['from'],\n",
    "#                 #         route_end = fix_price_zones['to']\n",
    "#                 #     )\n",
    "#                 # )\n",
    "#                 # conn.execute(stmt)\n",
    "#                 # conn.commit()\n",
    "#             except KeyError as e:\n",
    "#                 print(\"KeyError for route\")\n",
    "#                 continue\n",
    "#     \n",
    "#     try:\n",
    "#         pd.DataFrame(data=route_list, columns=['dispatch_id', 'route_start', 'route_end']).to_csv(result_csv, mode='a', header=False)\n",
    "#     except Exception as e:\n",
    "#         print(f\"csv: {e}\")\n",
    "#         continue\n",
    "#     \n",
    "#     # with sqlite_eng.begin() as conn:\n",
    "#     #     conn.execute(\n",
    "#     #         stmt, \n",
    "#     #         [\n",
    "#     #             {\n",
    "#     #                 'b_dispatch_id': i,\n",
    "#     #                 'route_start': r['route_start'],\n",
    "#     #                 'route_end': r['route_end']\n",
    "#     #             }\n",
    "#     #             for i,r in chunk.iterrows()\n",
    "#     #         ],\n",
    "#     #     )\n",
    "#     #     conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(result)\n",
    "# result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with sql_eng.begin() as conn:\n",
    "#     conn.execute(\n",
    "#         batch_stmt, \n",
    "#         [\n",
    "#             {\n",
    "#                 'b_dispatch_id': i,\n",
    "#                 'route_start': r['route_start'],\n",
    "#                 'route_end': r['route_end']\n",
    "#             }\n",
    "#             for i,r in res.iterrows()\n",
    "#         ],\n",
    "#     )\n",
    "#     conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chunk in pd.read_sql(query.statement, local_conn, chunksize=20):\n",
    "#     # [chunk[r] for r in chunk]\n",
    "#     # l = [r for r in chunk.iterrows()]\n",
    "#     # l\n",
    "#     for i,r in chunk.iterrows():\n",
    "#         # l = [r[0], r[1]['start_ltt'], r[1]['start_lng'], r[1]['end_ltt'], r[1]['end_lng']]\n",
    "#         params = {\n",
    "#             'from_lat': r['start_ltt'],\n",
    "#             'from_lng': r['start_lng'],\n",
    "#             'to_lat': r['end_ltt'],\n",
    "#             'to_lng': r['end_lng'],\n",
    "#         }\n",
    "#         try:\n",
    "#             response = requests.get(url=url, params=params)\n",
    "#         except requests.exceptions.Timeout:\n",
    "#             print('Timeout')\n",
    "#             continue\n",
    "#         except requests.exceptions.TooManyRedirects:\n",
    "#             print('TooManyRedirects')\n",
    "#             continue\n",
    "#             # Tell the user their URL was bad and try a different one\n",
    "#         except requests.exceptions.RequestException as e:\n",
    "#             print('RequestException, Catastrophic error!')\n",
    "#             continue\n",
    "#             # catastrophic error. bail.\n",
    "#             # raise SystemExit(e)\n",
    "#         \n",
    "#         except Exception as e:\n",
    "#             print(f\"request: {e}\")\n",
    "#             continue\n",
    "#         # print('2')\n",
    "#         try:\n",
    "#             res = response.json()\n",
    "#         except Exception as e:\n",
    "#             print(f\"json: {e}\")\n",
    "#             continue\n",
    "#         # print('3')\n",
    "#         try:\n",
    "#             fix_price_zones = res['fleets'][0]['vehicle_classes'][0]['price_detail']['base_pricing']['fix_price_detail']\n",
    "#         except KeyError as e:\n",
    "#             j = j +1\n",
    "#             print(f\"{j} Not Fixed Price!\")\n",
    "#             continue\n",
    "#         except IndexError as e:\n",
    "#             print(\"IndexError for fix_price_zones\")\n",
    "#             continue\n",
    "#         except Exception as e:\n",
    "#             print(f\"dict: {e}\")\n",
    "#             continue\n",
    "#         # print('4')\n",
    "#         if not isinstance(fix_price_zones,dict):\n",
    "#             print(f\"No fix price: {fix_price_zones}\")\n",
    "#         else:\n",
    "#             try:\n",
    "#                 # route = (fix_price_zones['from'], fix_price_zones['to'],pt._mapping['dispatch_id'])\n",
    "#                 # fix_zone_routes_list.append(route)\n",
    "#                 chunk.at[i, 'route_start']\n",
    "#                 \n",
    "#                 # ins = insert(fixed_zone_routes).values(\n",
    "#                 #     start=fix_price_zones['from'], end=fix_price_zones['from'], dispatch_id= pt        ._mapping['dispatch_id'])\n",
    "#                 i = i +1\n",
    "#             except KeyError as e:\n",
    "#                 print(\"KeyError for route\")\n",
    "#                 continue\n",
    "#         \n",
    "#         price_training_q = session.query(price_training_t).limit(500)\n",
    "# fix_zone_routes_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# j = 0\n",
    "# for pt in price_training_q:\n",
    "#     # print('1')\n",
    "#     # ride_samples.append(pt)\n",
    "#     params = {\n",
    "#         'from_lat': pt._mapping['start_ltt'],\n",
    "#         'from_lng': pt._mapping['start_lng'],\n",
    "#         'to_lat': pt._mapping['end_ltt'],\n",
    "#         'to_lng': pt._mapping['end_lng'],\n",
    "#         # 'from_utc':pt._mapping['from_utc'],\n",
    "#     }\n",
    "#     try:\n",
    "#         response = requests.get(url=url, params=params)\n",
    "#     except requests.exceptions.Timeout:\n",
    "#         print('Timeout')\n",
    "#         continue\n",
    "#     except requests.exceptions.TooManyRedirects:\n",
    "#         print('TooManyRedirects')\n",
    "#         # Tell the user their URL was bad and try a different one\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print('RequestException, Catastrophic error!')\n",
    "#         # continue\n",
    "#         # catastrophic error. bail.\n",
    "#         raise SystemExit(e)    \n",
    "#         \n",
    "#     except Exception as e:\n",
    "#         print(f\"request: {e}\")\n",
    "#         continue\n",
    "#     # print('2')\n",
    "#     try:\n",
    "#         res = response.json()\n",
    "#     except Exception as e:\n",
    "#         print(f\"json: {e}\")\n",
    "#         continue\n",
    "#     # print('3')\n",
    "#     try:\n",
    "#         fix_price_zones = res['fleets'][0]['vehicle_classes'][0]['price_detail']['base_pricing']['fix_price_detail']\n",
    "#     except KeyError as e:\n",
    "#         j = j +1\n",
    "#         print(f\"{j} Not Fixed Price!\")\n",
    "#         continue\n",
    "#     except IndexError as e:\n",
    "#         print(\"IndexError for fix_price_zones\")\n",
    "#         continue\n",
    "#     except Exception as e:\n",
    "#         print(f\"dict: {e}\")\n",
    "#         continue\n",
    "#     # print('4')\n",
    "#     if not isinstance(fix_price_zones,dict):\n",
    "#         print(f\"No fix price: {fix_price_zones}\")\n",
    "#     else:\n",
    "#         try:\n",
    "#             route = (fix_price_zones['from'], fix_price_zones['to'],pt._mapping['dispatch_id'])\n",
    "#             fix_zone_routes_list.append(route)\n",
    "#             # ins = insert(fixed_zone_routes).values(\n",
    "#             #     start=fix_price_zones['from'], end=fix_price_zones['from'], dispatch_id= pt._mapping['dispatch_id'])\n",
    "#             i = i +1\n",
    "#         except KeyError as e:\n",
    "#             print(\"KeyError for route\")\n",
    "#             continue\n",
    "# \n",
    "#         # print('5')\n",
    "#         if i%50 == 0:\n",
    "#             # connection.execute(ins)\n",
    "#             df = pd.DataFrame(fix_zone_routes_list, columns=['start', 'end', 'dispatch_id'])\n",
    "#             df.to_sql('fixed_zone_routes', sqlite_eng, if_exists='append')\n",
    "#             fix_zone_routes_list = []\n",
    "#             print(f\"Created {i} records\")\n",
    "# \n",
    "#     # print('6')\n",
    "# df = pd.DataFrame(fix_zone_routes_list, columns=['start', 'end', 'dispatch_id'])\n",
    "# df.to_sql('fixed_zone_routes', sqlite_eng, if_exists='append')\n",
    "# fix_zone_routes_list = []\n",
    "# print(f\"Created {i} records\")\n",
    "#     # print('6')\n",
    "#     # print(pt._mapping['ride_id'], pt._mapping['dispatch_amount'], pt._mapping['dispatch_currency'])\n",
    "#     # # print(pt._mapping['start_place'])\n",
    "#     # print(pt._mapping['start_lng'], pt._mapping['start_ltt'])\n",
    "#     # # print(pt._mapping['end_place']) \n",
    "#     # print(pt._mapping['end_lng'], pt._mapping['end_ltt'])\n",
    "#     # print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use ORM to retrieve records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_count = session.query(F.count(price_training_t.c.ride_id)).scalar()\n",
    "# print(sample_count)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://j1j495o5pk.execute-api.us-east-2.amazonaws.com/upncoming/ride-pricings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# params = {\n",
    "#     'from_lat': 37.61911449999999,\n",
    "#     'from_lng':-122.3816274,\n",
    "#     'to_lat':37.3635295,\n",
    "#     'to_lng':-121.9285932,\n",
    "#     'from_utc':1727352000,\n",
    "# }\n",
    "# response = requests.get(url=url, params=params)\n",
    "# response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = response.json()\n",
    "# fix_price_zones = res['fleets'][0]['vehicle_classes'][0]['price_detail']['base_pricing']['fix_price_detail']\n",
    "# print(fix_price_zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ride_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt = price_training_q.first()\n",
    "# pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'from_lat': (pt._mapping['start_ltt']),\n",
    "#     'from_lng': (pt._mapping['start_lng']),\n",
    "#     'to_lat': (pt._mapping['end_ltt']),\n",
    "#     'to_lng': (pt._mapping['end_lng']),\n",
    "#     # 'from_utc': int(pt._mapping['from_utc']),\n",
    "# }\n",
    "# params\n",
    "\n",
    "# response = requests.get(url=url, params=params)\n",
    "# res = response.json()\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import String,Integer,insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix_zone_routes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_training_q = session.query(price_training_t).limit(10)\n",
    "# ride_samples = []\n",
    "# for pt in price_training_q:\n",
    "#     ride_samples.append(pt)\n",
    "#     print(pt._mapping['ride_id'], pt._mapping['dispatch_amount'], pt._mapping['dispatch_currency'])\n",
    "#     # print(pt._mapping['start_place'])\n",
    "#     print(pt._mapping['start_lng'], pt._mapping['start_ltt'])\n",
    "#     # print(pt._mapping['end_place']) \n",
    "#     print(pt._mapping['end_lng'], pt._mapping['end_ltt'])\n",
    "#     print(\"------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ride_samples[0]._mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(ride_samples)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rides_q = session.query(price_training_t).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for ride in rides_q:\n",
    "#     print(ride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rides_q = session.query(ride_ride_t).limit(10)\n",
    "# for ride in rides_q:\n",
    "#     print(ride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(price_training_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use core to retrieve records\n",
    "# rp = connection.execute(price_training_t)\n",
    "# for i, record in enumerate(rp):\n",
    "#     print(i, record.ride_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = rp.fetchall()\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_sql(price_training_t, engine)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlite_eng = create_engine('sqlite:///../data/price_training_from_gbq_raw.db', echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_sql('price_training_orlando_mpv5', sqlite_eng, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic peak time exclusion\n",
    "source https://www.quora.com/What-is-the-trickiest-time-of-the-day-to-drive-in-Orlando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from workalendar.usa import Florida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal_orlando = Florida()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_dates = [d[0] for d in cal_orlando.holidays(2024)]\n",
    "# exclude_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from workalendar.usa import Florida \n",
    "# import numpy as np\n",
    "# cal_florida = Florida()\n",
    "# exclude_dates_str = [str(d[0]) for d in cal_florida.holidays(2024)]\n",
    "# exclude_dates_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_dates = [d[0] for d in cal_florida.holidays(2024)]\n",
    "# exclude_dates\n",
    "# res = df['from_datetime_utc'].apply(lambda x: x in exclude_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.any(res.apply(lambda x: x in exclude_dates))\n",
    "# res[0]=True\n",
    "# res\n",
    "# np.any(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pricing.data.utils import validate_datetime_in_iso_format, validate_timezone_in_iana, get_timezone_abbreviation, fix_timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_invalid_datetime = df[df.apply(lambda x: not validate_datetime_in_iso_format(x['from_datetime_fix_str']), axis=1)]\n",
    "# df_invalid_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_invalid_timezone = df[df.apply(lambda x: not validate_timezone_in_iana(x['from_timezone_str']), axis=1)]\n",
    "# df_invalid_timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orlando_airport = pd.read_csv('../../data/orlando_all_output.csv')\n",
    "#orlando_airport.head()\n",
    "#orlando_airport.dtypes\n",
    "#orlando_airport.to_sql('orlando_airport', sqlite_eng, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['from_timezone_fix_str'] = df.apply(lambda x: fix_timezone(x['from_timezone_str']), axis=1) \n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_valid_datetime = df[df.apply(lambda x: validate_datetime_in_iso_format(x['from_datetime_fix_str']), axis=1)]\n",
    "# df_valid_timezone = df_valid_datetime[df_valid_datetime.apply(lambda x: validate_timezone_in_iana(x['from_timezone_fix_str']), axis=1)]\n",
    "# df_valid_timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_valid_timezone.loc[df_invalid_timezone.index, ['from_timezone_str', 'from_timezone_fix_str']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# import pytz\n",
    "# df_utc_offset = (df_valid_timezone.apply(lambda x: datetime.now(pytz.timezone(x['from_timezone_fix_str'])), axis=1)\n",
    "#                             .apply(lambda x: x.strftime('%z')))\n",
    "# df_utc_offset.name = 'utc_offset'\n",
    "# df_utc_offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dt_str = df_valid_timezone['from_datetime_fix_str']\n",
    "# df_valid_timezone['from_datetime_local'] = df_valid_timezone.apply(lambda x: (pd.to_datetime(x['from_datetime_fix_str']).to_datetime64()), axis=1)\n",
    "# df_valid_timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid_timezone.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_valid_timezone['from_datetime_local_tz'] = df_valid_timezone.apply(lambda x: pytz.timezone(x.loc[:,'from_timezone_str']).localize(x.loc[:,'from_datetime_local']), axis=1)\n",
    "# df_valid_timezone['from_timezone'] = df_valid_timezone.apply(lambda x: pytz.timezone(x['from_timezone_fix_str']), axis=1)\n",
    "# df_valid_timezone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid_timezone['from_datetime_tz'] = df_valid_timezone.apply(lambda x: x['from_timezone'].localize(x['from_datetime_local']), axis=1)\n",
    "# df_valid_timezone\n",
    "                                    # .apply(lambda x: x.localize(x.loc[:,'from_timezone_str']), axis=1))\n",
    "# df_valid_timezone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filtering out peak traffic time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak_time_str = [('07:00:00', '09:00:00'), ('16:00:00', '19:00:00')]\n",
    "# night_time_str = [('22:00:00', '6:00:00')]  # Shanghai, US usually no overtime extra fees New York 8pm ~ 6am\n",
    "# ind = []\n",
    "# td = []\n",
    "# for pt in peak_time_str:\n",
    "#     ind.append(pd.DatetimeIndex(pt))\n",
    "# ind\n",
    "# for i in ind:\n",
    "#     i[1]-i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak_time = []\n",
    "# for pt in peak_time_str:\n",
    "#     peak_time.append(pd.date_range(pt[0], pt[1], freq='h'))\n",
    "# for pt in peak_time:\n",
    "#     print(pt, pt.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak_time_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_peak_traffic_time = df_valid_timezone[\n",
    "#     df_valid_timezone['from_datetime_tz'].apply(lambda x: x.strftime('%H:%M:%S')).between(*peak_time_str[0])\n",
    "#     | df_valid_timezone['from_datetime_tz'].apply(lambda x: x.strftime('%H:%M:%S')).between(*peak_time_str[1])\n",
    "# ]\n",
    "# df_peak_traffic_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_out_of_peak_traffic_time = df_valid_timezone[\n",
    "#     ~ (df_valid_timezone['from_datetime_tz'].apply(lambda x: x.strftime('%H:%M:%S')).between(*peak_time_str[0])\n",
    "#     | df_valid_timezone['from_datetime_tz'].apply(lambda x: x.strftime('%H:%M:%S')).between(*peak_time_str[1]))\n",
    "# ]\n",
    "# df_out_of_peak_traffic_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_utc_offset = (df_valid_timezone.apply(lambda x: datetime.now(pytz.timezone(x.loc['timezone'])))\n",
    "#                             .apply(lambda x: x.strftime('%z')))\n",
    "# df_utc_offset.name = 'utc_offset'\n",
    "# df_utc_offset\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter out round trip (time reservation) with feature distance = 1 (m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_no_round_trip = df_valid_timezone[df_valid_timezone['distance'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_training = df_no_round_trip.loc[:,['ride_id', 'trip_type', 'trip_no', 'trip_count', 'ride_status', 'partner', 'fleet', \n",
    "#                 'start_place', 'end_place',\n",
    "#                 'passenger_count', 'luggage_count',\n",
    "#                 'dispatch_amount', 'dispatch_currency',\n",
    "#                 'distance', 'duration', 'vehicle_class', \n",
    "#                 'from_datetime_tz']]\n",
    "#  \n",
    "# df_training['cent_price_per_km'] = df_training['dispatch_amount'] / df_training['distance']*100.0\n",
    "# df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def average_cent_per_km(x):\n",
    "#     x['average_cent_per_km'] = x['cent_price_per_km'].mean()\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fleet_trip_no(x):\n",
    "#     x['fleet_trip_count'] = len(x)\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fleet_statistics = df_training.loc[:, ['ride_id','fleet']]\n",
    "# df_fleet_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_fleet_trip_no = df_fleet_statistics.groupby('fleet').aggregate([len])\n",
    "# df_fleet_trip_no.sort_values(by=('ride_id', 'len'), ascending=False, inplace=True)\n",
    "# df_fleet_trip_no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_big_fleets = df_fleet_trip_no[df_fleet_trip_no[('ride_id','len')] >100]\n",
    "# df_big_fleets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_big_fleet_data = []\n",
    "# for f in df_big_fleets.index:\n",
    "#     print(f)\n",
    "#     df_big_fleet_data.append(df_training[df_training['fleet'] == f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_big_fleet_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_big_fleet_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_big_fleet_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # df_training['average_cent_per_km'] = df_training.groupby('fleet').apply(average_cent_per_km)\n",
    "# df_analysis = df_training.groupby('fleet').apply(average_cent_per_km)\n",
    "# df_analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
